import os
import torch
import streamlit as st
import logging
import sqlite3
from datetime import datetime
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from ollama import chat
from transformers import pipeline

# ---------------------- Logging ----------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ---------------------- Database ----------------------
def init_db():
    conn = sqlite3.connect("questions.db")
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS questions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            question TEXT NOT NULL,
            answer TEXT NOT NULL,
            timestamp TEXT NOT NULL
        )
    """)
    conn.commit()
    conn.close()
    logging.info("üì¶ Database initialized successfully.")

def save_question_to_db(question, answer):
    conn = sqlite3.connect("questions.db")
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO questions (question, answer, timestamp)
        VALUES (?, ?, ?)
    """, (question, answer, datetime.now().isoformat()))
    conn.commit()
    conn.close()
    logging.info("‚úÖ Saved question to database.")

# ---------------------- Intent Classification ----------------------
st.set_page_config(page_title="RAG Chatbot ‡∏Å‡∏¢‡∏®", page_icon="üìÑ")
st.write("üí° ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Intent Classification...")
classifier = pipeline("text-classification", model="bhadresh-savani/bert-base-uncased-emotion")

intent_map = {
    "joy": "GREETING",
    "love": "GREETING",
    "surprise": "GREETING",
    "sadness": "QUESTION",
    "anger": "QUESTION",
    "fear": "QUESTION"
}

def classify_intent(text: str) -> str:
    result = classifier(text)[0]
    label = result['label']
    return intent_map.get(label, "OTHER")

# ---------------------- Keyword Filter ----------------------
GREETING_KEYWORDS = [
    "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞", "hello", "hi", "hey", "‡∏´‡∏ß‡∏±‡∏î‡∏î‡∏µ",
    "good morning", "good afternoon", "good evening",
    "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏∞",
    "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©", "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡πà‡∏∞",
    "‡πÇ‡∏≠‡πÄ‡∏Ñ", "ok", "‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°", "‡∏î‡∏µ‡∏°‡∏≤‡∏Å", "great",
    "‡∏Æ‡πà‡∏≤", "‡∏Æ‡πà‡∏≤ ‡πÜ", "lol", "haha", "hehe",
    "‡πÉ‡∏ä‡πà", "‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà", "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ñ‡πà‡∏∞", "‡πÄ‡∏≠‡∏≠", "‡∏≠‡∏∑‡∏°", "‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠", "‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏´‡∏£‡∏≠"
]

def should_store_question(text: str) -> bool:
    if any(word.lower() in text.lower() for word in GREETING_KEYWORDS):
        return False
    intent = classify_intent(text)
    if intent in ["GREETING", "OTHER"]:
        return False
    return True

# ---------------------- RAG Pipeline ----------------------
st.write("üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£...")
loader = UnstructuredFileLoader("Loan_Features.docx")
docs = loader.load()

if not docs or not docs[0].page_content.strip():
    st.error("‚ùå ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    st.stop()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)
logging.info(f"‚úÖ Document split into {len(chunks)} chunks")

embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)
logging.info("üí° Embedding model loaded")

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="chroma_db"
)
logging.info("üìö Vector store created successfully")

def retrieve(query: str):
    return vectorstore.similarity_search(query, k=3)

def generate_answer(user_query: str, context: str) -> str:
    messages = [
        {
            "role": "system",
            "content": (
                "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡∏Å‡∏¢‡∏® "
                "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
                "‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏Ñ‡∏£‡∏±‡∏ö"
            )
        },
        {
            "role": "user",
            "content": (
                f"Context:\n{context}\n\n"
                "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n"
                "Q: ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà‡∏ï‡πà‡∏≠‡∏õ‡∏µ?\n"
                "A: ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 360,000 ‡∏ö‡∏≤‡∏ó‡∏ï‡πà‡∏≠‡∏õ‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö\n"
                "Q: ‡∏Ñ‡∏ô‡∏≠‡∏≤‡∏¢‡∏∏ 33 ‡∏õ‡∏µ ‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?\n"
                "A: ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà 4 (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 35 ‡∏õ‡∏µ)\n\n"
                f"Q: {user_query}\nA:"
            )
        }
    ]
    response = chat(model="llama3.2:latest", messages=messages)
    return response["message"]["content"]

# ---------------------- Streamlit Chat UI ----------------------
st.title("üìÑ RAG Chatbot ‡∏Å‡∏¢‡∏® (ChatGPT-style)")

init_db()

# ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ä‡∏ó‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
if "messages" not in st.session_state:
    st.session_state.messages = []

user_input = st.chat_input("‚ùì ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡∏Å‡∏¢‡∏®:")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    with st.spinner("üìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á..."):
        retrieved_docs = retrieve(user_input)
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        logging.info("üîç Retrieved relevant context")
    
    with st.spinner("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
        answer = generate_answer(user_input, context)
        logging.info("‚úÖ Answer generated")
    
    st.session_state.messages.append({"role": "assistant", "content": answer})
    
    if should_store_question(user_input):
        save_question_to_db(user_input, answer)
        st.success("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    else:
        st.info("‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô Greeting / Small Talk")

# ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ä‡∏ó‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ö‡∏ö ChatGPT
for msg in st.session_state.messages:
    if msg["role"] == "user":
        with st.chat_message("user"):
            st.markdown(msg["content"])
    else:
        with st.chat_message("assistant"):
            st.markdown(msg["content"])
